\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{framed}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\definecolor{shadecolor}{gray}{.95}
\setlength{\parindent}{0pt}
\theoremstyle{definition}
\newtheorem*{answer}{Answer}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\newcommand{\code}[1]{\texttt{#1}}

\title{Literature Review}
\author{Jeff Hara (jhara18)}

\begin{document}
\maketitle

\section{Problem Definition}

\quad Semantic parsing is broadly defined as the problem of mapping natural language to a logical form. This has been an open problem for some time, and there have been a wide array of approaches to semantic parsing over the years. Since the late 60's, a wide array of studies sought to develop semantic parsing to extract the meaning representations from natural language for question-answering systems\cite{Woods77}. However, developing hand-built mappings from natural language to logic was too daunting a task at the time, so interest in this problem died out.

\quad At the turn of the century, a renewed interest in machine learning rekindled as well the promise of semantic parsing. Semantic parsing is fundamentally a translation problem. If we can reliably reduce natural language, which is often ungrammatical and verbose, to a precise, concise logical form, it would expand the capabilities of Natural Language Understanding tremendously. This is because the task of semantic parsing is doing several things at once. First, semantic parsing is a type of dimensionality reduction. Many natural language expressions can share the same logical form, so we can extract a simplified meaning representation. Second, logical forms are machine-interpretable, and often semantic parsers translate natural language to an executable query language. Finally, logical forms have a natural hierarchical interpretation, allowing us to infer grammatical patterns and relations.

\quad Semantic parsing is still extremely relevant, and there are many ways that we can exploit logical forms. One popular use-case has been in question-answering tasks. A question can be semantically parsed into a query to a knowledge base, and the query is executed to retrieve the answer. Some papers have evaluated the translated first-order logic expressions as a truth-value \cite{jha2011}. As semantic parsing systems develop, we can expect AI to become more clever and make better inferences.

\quad In the current field, we are seeing a paradigm-shift from shallow  to deep semantic parsing, and we will take a brief survey of the various approaches to semantic parsing over the last ten years.

\section{Overview of Semantic Parsing Strategies}

\quad There have been a wide array of approaches to semantic parsing over the years. From the late 60's, there have been studies that attempted to define rules to perform this mapping to logical forms for question answering systems \cite{Woods77}, but as we can imagine, creating hand-built mappings was not robust nor scalable.

\quad Instead, learning these mappings in a data-driven manner proved to be successful, and we will be taking a look at some of these approaches. We will start by looking at the work by Raymond Mooney of University of Texas at Austin, whose prolific contributions to semantic parsing have been monumental. Then, we will evaluate an unsupervised approach, which offers promising areas for development. Finally, we will look at the most recent developments in semantic development.

\section{Mooney's work in semantic parsing}

We will look at two papers from Mooney and his students, Rohit Kate and Yuk Wah Wong.

\quad In Kate et al. (2005), they present a semantic parser called SILT (Semantic Interpretation by Learning Transformations), and this system is essentially a rule-based parser, where all of the rules were learned through training. Given an arbitrary formal language with deterministic context-free grammars, SILT can induce transformation rules from a set of natural language sentences and their corresponding logical forms. They demonstrate two variations of SILT; one is string-based and the other tree-based. This approach is quite similar to feature extraction, albeit slightly more crude. In the string-based version, SILT would essentially define regular expressions that could match natural language sentences that mapped to the same logical form, and similarly, the tree-based version would try to match subtree patterns.

\quad The results of SILT were not that ground-breaking, however. In a comparison against Mooney's other ILP (Inductive Logic Programming) systems CHILL (Zelle \& Mooney 1996) and COCKTAIL (Tang \& Mooney 2001) on the GEOQUERY dataset, the string-based SILT trained the fastest but suffered from poor recall, and the tree-based SILT had a competitive F1 score to COCKTAIL with a slight edge on recall to CHILL. Both SILTs were more precise than COCKTAIL, and they had the advantage of being language-agnostic with respect to the logical form.

\quad In general, ILP-based systems had the problem of being too idiosyncratic, and this paper was a push to generalize semantic parsing systems. This was taken a step further in Wong \& Mooney (2007), where they proposed a statistical machine translation system for question-answering called $\lambda$-WASP. This system uses lambda calculus as their target language, and this logical form has the added benefit of having logical variables.

\quad Similarly to the SILT system, there was a process of rule extraction. However, since they were using lambda calculus, they were able to leverage a more clever approach to rule extraction. The logical variables indicated which words in the natural language linked to which words in the logical form, so the $\lambda$-WASP algorithm included rules to restructure the logical form to be optimally isomorphic to the natural language. As an example, the question ``How many states border Texas?'' is logically equivalent to the question ``How many bordering states does Texas have?,'' but the order of information is different. In previous work, this would have lead to the system extracting two different rules, but $\lambda$-WASP could create more generalizable rules that were robust to these types of natural language variations.

\quad The $\lambda$-WASP system was very performant. On the GEOQUERY dataset, it had an F1-score of $89.19$, a $9$ point improvement on the next best system, with high precision and recall. The regrouping algorithm described above boosted precision by $1.3\%$ and recall by $4.2\%$. Additionally, $\lambda$-WASP required no previous knowledge of the natural language and boasted comparable performance on the Spanish, Japanese, and Turkish GEOQUERY datasets.

\quad Overall, in the span of a decade or so, several iterations of semantic parsers were developed, and there was a paradigm-shift from hand-writing rules about language to learning the rules statistically. The advantage of learning the rules statistically is that one need not make assumptions about the source language, so it is easily scalable. One of the promising target languages was lambda-calculus, which is a cousin of first-order logic.

\quad The main issue with these models is that their rule extraction methods depend on having the ``gold'' logical forms, and this data is relatively hard to come by since it must be human-generated. We will now look at another approach that tries to tackle this problem in semantic parsing.

\section{An Unsupervised Approach}

Thus far, most machine learning approaches to semantic parsing were supervised, which led to issues in scalability. Poon \& Domingos (2009) developed the first unsupervised semantic parser (USP) to rectify this issue.

\quad The main idea behind unsupervised semantic parsing is to parse a natural language into its dependency tree and formulate what they call QLFs (Quasi-Logical Forms); from these QLFs, they cluster similar QLFs and produce the generalized logical forms. The process to convert the dependency tree to the QLF is defined; the clustering is what makes this unsupervised, and this is an analog of the rule-extraction from Mooney's work.

\quad First, to get the QLFs, we must break down a sentence into atoms. This step can be done easily from our dependency trees, since the leaf nodes of a sentence have a natural atomic interpretation and edges can be interpreted as predications. For a sentence like ``Utah borders Idaho,'' each word (leaf node) becomes its own atom, ``Utah($n_1$),'' ``borders($n_2$),'' and ``Idaho($n_3$),'' and each subject dependency arc becomes its own predicate, ``nsubj($n_2$, $n_1$)'' and ``dobj($n_2$, $n_3$).'' The QLF for the sentence would be the conjunction (\&) of these atoms. As we can see, the conversion from a dependency parse tree is technically defined but not in any meaningful way. We can think of the conversion to the QLF as another representation of a parse tree, rather than its own separate thing.

\quad The idea is that a subset of the QLF atoms will correspond to a semantic relation. For example, ``borders($n_2$) \& nsubj($n_2$, $n_1$) \& dobj($n_2$, $n_3$)'' would represent the next-to relation. This demonstrates one significant departure from Mooney's work. In $\lambda$-WASP, the restructuring algorithm was used to maintain isometry in cases where the words in the natural language were out of order with respect to the logical form. However, unsupervised semantic parsing side-steps this issue altogether by defining a semantic relation as a subset of the QLF atoms without order. Thus, the subset ``nsubj($n_2$, $n_1$) \& dobj($n_2$, $n_3$) \& borders($n_2$),'' a permutation of the previous relation, is treated as the same relation. Conceptually, when we cluster the QLFs, we want to make clusters where the QLFs in a given cluster are interchangeable.

\quad Thus, for any sentence, we can partition its QLFs into parts and assign those parts to clusters, which are used to build the final logical form. The parsing algorithm iteratively chooses partitions and variable assignments and outputs the optimal partition and assignments for a sentence. <NEED TO REREAD SECTION V...>

\quad For evaluation, they tested USP's performance in question-answering on $2000$ hand-written natural language sentences with a biomedical dataset GENIA. Without a labeled set of logical forms, USP could not be compared against the state-of-the-art supervised models, but they did use other question-answering systems that were quite primitive. USP did significantly better than the next best model, but since the baselines were low-performing, it is difficult to really evaluate how good the USP model is. For recall, they do show figures that USP answers more questions than the baselines, but they do not share how many questions were used in testing, so its recall may be low. Its reported accuracy was $88\%$, which was significantly better than the other baseline approaches. The paper does identify a few weaknesses; the clustering can be insensitive to antonym relations, and there is limited complexity in lambda relations.

\quad In summary, it seems that this paper is a solid proof of concept, but without the comparisons against the supervised models, its viability is uncertain. What it does offer is a model that can generate logical forms from natural language without any labeling, and it may prove useful for generating datasets for use in distantly supervised problems. The conversion from natural language into a QLF may be a useful intermediate step as a translation problem, but it is not conceptually that different from a parse tree. One weakness that is apparent in Mooney but subdued in Poon et al. is that the extracted rules and the QLF clusters are very dependent on the specific lexicon. The SILT and the USP systems are, in a sense, generalizing keyword matching, albeit in complex ways, and there may be something undesirable about doing this. The $\lambda$-WASP system was language agnostic, and this is really interesting feature, especially if we applied semantic parsing to foreign language translation. Thus, in our next paper, we will return to the idea of being language agnostic, using neural machine translation techniques.

\section{Deep Learning Techniques in Semantic Parsing}

In Dong \& Lapata (2016), we have a direct application of neural machine translation techniques to semantic parsing. In systems like SILT and USP, if ``borders'' is in the input, ``borders'' or something similar would be the predicate in the target language. Instead, Dong \& Lapata avoid making assumptions about the lexicons and features since they may not generalize across domains, and they simply treat the logical form as a language unto itself.

\quad This lends itself very well to neural machine translation. Dong and Lapata experiment with a vanilla implementation of a sequence-to-sequence model, which uses a multi-layer LSTM encoder and decoder with word embeddings. They also tried a sequence-to-tree model to address the fact that a Seq2Seq model ignores any hierarchical relationship that exists in grammar. The sequence-to-tree model uses the same encoder, but for the decoder, it recursively expands a subtree when it predicts a nonterminal term. They also experiment with an attention mechanism, such that the decoded output is a function of the input vectors as well as its hidden state. We can think of the attention mechanism as an analog to what SILT is doing with keyword-matching.

\quad This paper uses four common datasets, JOBS (database of job listings), GEOQUERY, ATIS (flight bookings), and IFTTT (inferences), and each dataset uses different types of natural language inputs and logical form outputs. For example, ATIS's inputs are simple commands (``dallas to san francisco leaving after 4 in the afternoon please''), whereas GEOQUERY's inputs are questions (``what is the population of the state with the largest area''). Impressively, the Seq2Seq and Seq2Tree models both performed very competitively across all the datasets, in spite of not applying any domain-specific knowledge or language-specific heuristics. This paper only compared accuracies, so it would be interesting to see if its overall F-1 score was good as well.

\section{State-of-the-Art Semantic Parsing}



\section{Future Work}



    General problem/task definition: What are these papers trying to solve? Why?
    Concise summaries of the articles: Do not simply copy the article text in full. We can read them ourselves. Put in your own words the major contributions of each article.
    Compare and contrast: Point out the similarities and differences of the papers. Do they agree with each other? Are results seemingly in conflict? If the papers address different subtasks, how are they related? (If they are not related, then you may have made poor choices for a lit review...). This section is probably the most valuable for the final project.
    Future work: Make several suggestions for how the work can be extended. Are there open questions to answer? This would presumably include how the papers relate to your final project idea.

{\small
\bibliographystyle{ieee}
% \bibliography{egbib}
\begin{thebibliography}{9}
	\bibitem{jha2011}
    Jha et al.
    \\\texttt{http://www-personal.umich.edu/\~rahuljha/}

    \bibitem{Woods77}
    Woods, WA.
    \\\texttt{https://www.bibsonomy.org/bibtex/1fb270a4a4c74210bc64b6d16161502c}
\end{thebibliography}
}

\end{document}
